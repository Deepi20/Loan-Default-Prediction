import os
import re
import pandas as pd
import numpy as np
from datetime import datetime

import warnings
warnings.filterwarnings('ignore')

from sklearn.preprocessing import LabelEncoder

#from imblearn.combine import SMOTETomek

from sklearn.model_selection import train_test_split

from sklearn.model_selection import StratifiedKFold

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from imblearn.combine import SMOTETomek
from sklearn.model_selection import GridSearchCV
import pandas 
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

df_train= pandas.read_csv("C:/Users/New folder/financial loan/train_aox2Jxw/train.csv")
df_test = pandas.read_csv("C:/Users/New folder/financial loan/train_aox2Jxw/test_bqCt9Pv.csv")

df_train

df_train.columns
df_test.columns

sns.set(style="white")

# Generate a large random dataset
temp3 = df_train.copy() # our dataframe

# Compute the correlation matrix
corr = temp3.corr() # corr calculation

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(15, 15))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5})
plt.show()

df_train.nunique()

print(df_train.MobileNo_Avl_Flag.var())
print(df_test.MobileNo_Avl_Flag.var())

UniqueID = [df_test.UniqueID]
df_train.drop(['UniqueID','MobileNo_Avl_Flag'], axis=1, inplace=True)
df_test.drop(['UniqueID','MobileNo_Avl_Flag'], axis=1, inplace=True)

d_marker = '21-04-19'
def days_between(d1,d2):
    d1 = datetime.strptime(d1,"%d-%m-%y")
    d2 = datetime.strptime(d2,"%d-%m-%y")
    return abs((d2-d1).days)
    
 df_train['Date.of.Birth'] = df_train['Date.of.Birth'].apply(lambda x:  days_between(x,d_marker)/365)
# Calculating time (in yrs) after disbursal
df_train['DisbursalDate']= df_train['DisbursalDate'].apply(lambda x:  days_between(x,d_marker)/365)
# age as on 1-1-2019 (in yrs)
df_test['Date.of.Birth'] = df_test['Date.of.Birth'].apply(lambda x:  days_between(x,d_marker)/365)
# Calculating time (in yrs) after disbursal
df_test['DisbursalDate']= df_test['DisbursalDate'].apply(lambda x:  days_between(x,d_marker)/365)

df_train['CREDIT.HISTORY.LENGTH']= df_train['CREDIT.HISTORY.LENGTH'].apply(lambda x: (re.sub('[a-z]','',x)).split())
df_train['CREDIT.HISTORY.LENGTH']= df_train['CREDIT.HISTORY.LENGTH'].apply(lambda x: int(x[0])*12+int(x[1]))

# Converting the given 'AVERAGE.ACCT.AGE' in months
df_train['AVERAGE.ACCT.AGE']= df_train['AVERAGE.ACCT.AGE'].apply(lambda x: (re.sub('[a-z]','',x)).split())
df_train['AVERAGE.ACCT.AGE']= df_train['AVERAGE.ACCT.AGE'].apply(lambda x: int(x[0])*12+int(x[1]))

# Converting the given 'CREDIT.HISTORY.LENGTH' in months
df_test['CREDIT.HISTORY.LENGTH']= df_test['CREDIT.HISTORY.LENGTH'].apply(lambda x: (re.sub('[a-z]','',x)).split())
df_test['CREDIT.HISTORY.LENGTH']= df_test['CREDIT.HISTORY.LENGTH'].apply(lambda x: int(x[0])*12+int(x[1]))

# Converting the given 'AVERAGE.ACCT.AGE' in months
df_test['AVERAGE.ACCT.AGE']= df_test['AVERAGE.ACCT.AGE'].apply(lambda x: (re.sub('[a-z]','',x)).split())
df_test['AVERAGE.ACCT.AGE']= df_test['AVERAGE.ACCT.AGE'].apply(lambda x: int(x[0])*12+int(x[1]))

cat_cols= ['branch_id','supplier_id', 'manufacturer_id', 'Current_pincode_ID', 'Employment.Type', 'State_ID', 'Employee_code_ID', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag','Driving_flag', 'Passport_flag','PERFORM_CNS.SCORE.DESCRIPTION']

num_cols= ['disbursed_amount', 'asset_cost', 'ltv','Date.of.Birth','DisbursalDate', 'PERFORM_CNS.SCORE', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS',
       'PRI.OVERDUE.ACCTS', 'PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT',
       'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS',
       'SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE', 'SEC.SANCTIONED.AMOUNT',
       'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT', 'SEC.INSTAL.AMT',
       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',
       'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'NO.OF_INQUIRIES']
       
 df_train[cat_cols].head()
 df_train.isnull().sum()
 
 df_train['Employment.Type'].value_counts()
 
print('% of Employment type missing values is {0}% for the training set'.format(round(100*df_train['Employment.Type'].isnull().sum()/len(df_train),2)))
print('% of Employment type missing values is {0}% for the test set'.format(round(100*df_test['Employment.Type'].isnull().sum()/len(df_test),2)))
df_train.fillna('unknown', inplace=True)
df_test.fillna('unknown', inplace=True)
df_train['Employment.Type'].value_counts()

pd.concat([df_train.isnull().sum(),df_test.isnull().sum()], sort= False, axis=1)

le = LabelEncoder()
df_train.iloc[:,8] = le.fit_transform(df_train.iloc[:,8])
df_train.iloc[:,18] = le.fit_transform(df_train.iloc[:,18])
df_test.iloc[:,8] = le.fit_transform(df_test.iloc[:,8])
df_test.iloc[:,18] = le.fit_transform(df_test.iloc[:,18])

for i in num_cols:
    df_train[i] = (df_train[i]- min(df_train[i])) / (max(df_train[i])- min(df_train[i]))
    df_test[i] = (df_test[i]- min(df_test[i])) / (max(df_test[i])- min(df_test[i]))
    
X= df_train.drop('loan_default', axis=1)
y= df_train.loan_default

X= pd.concat([X[num_cols], pd.get_dummies(X[cat_cols], drop_first=True)], sort=False, axis=1)
smt = SMOTETomek(ratio='auto')
X_smt, y_smt = smt.fit_sample(X, y)
cv =StratifiedKFold(n_splits=10,shuffle=True,random_state=45)
rf= RandomForestClassifier(n_estimators=300,verbose=1, n_jobs=-1,random_state=42)
rf.fit(X_smt,y_smt)

pre_out= rf.predict_proba(df_test)
pre_out
correlation = df_train.corr()
plt.figure(figsize=(16,16))
sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')

plt.title('Correlation between different features')
X= df_train.drop('VoterID_flag', axis=1)
X= df_train.drop('PERFORM_CNS.SCORE.DESCRIPTION', axis=1)

X= df_train.drop(['VoterID_flag','PERFORM_CNS.SCORE.DESCRIPTION'],axis=1)
X.columns
correlation = X.corr()
plt.figure(figsize=(16,16))
sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')

plt.title('Correlation between different features')

rf= RandomForestClassifier(n_estimators=300,verbose=1, n_jobs=-1,random_state=42)
pre_rf = cross_val_predict(rf, cv=cv, X=X_smt,y=y_smt, verbose=1,method='predict_proba')

print("auc score =\t" ,roc_auc_score(y_smt, pre_rf[:,1]))

rf_f= RandomForestClassifier(n_estimators=300,verbose=1, n_jobs=-1,random_state=42)
rf.fit(X_smt,y_smt)
pre_out = rf.predict_proba(df_test)[:,1]

df_test.head()

df_loan=pd.DataFrame(pre_out, columns=['loan_default']) 
df_loan

df_loan.to_csv('C:/Users/deepi/OneDrive/Documents/ammapls/New folder/financial loan/train_aox2Jxw/solution.csv',index=False)



